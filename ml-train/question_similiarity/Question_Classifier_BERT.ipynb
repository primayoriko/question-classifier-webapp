{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Question_Classifier_BERT.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phdDAezkyY9F"
      },
      "source": [
        "# Question Classifier using BERT\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Na-uI9QMURF",
        "outputId": "c006d877-f887-45a8-e4dc-b4ef70e9a281"
      },
      "source": [
        "!pip install -q kaggle\n",
        "!pip install transformers==3.0.0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers==3.0.0 in /usr/local/lib/python3.7/dist-packages (3.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (21.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2019.12.20)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.1.96)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.0.46)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (3.3.2)\n",
            "Requirement already satisfied: tokenizers==0.8.0-rc4 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (0.8.0rc4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (1.19.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.0) (2.23.0)\n",
            "Requirement already satisfied: pyparsing<3,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.0) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.0) (2021.10.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.0) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VP6DVthMgRI"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pqf0ujeCMk7F",
        "outputId": "24f4d064-9cd1-44cb-c918-dcf20a217177"
      },
      "source": [
        "!mkdir ~/.kaggle\n",
        "\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n",
        "!kaggle datasets list"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "ref                                                         title                                              size  lastUpdated          downloadCount  \n",
            "----------------------------------------------------------  ------------------------------------------------  -----  -------------------  -------------  \n",
            "gpreda/reddit-vaccine-myths                                 Reddit Vaccine Myths                              237KB  2021-11-10 18:11:20          15343  \n",
            "crowww/a-large-scale-fish-dataset                           A Large Scale Fish Dataset                          3GB  2021-04-28 17:03:01           9331  \n",
            "imsparsh/musicnet-dataset                                   MusicNet Dataset                                   22GB  2021-02-18 14:12:19           4171  \n",
            "dhruvildave/wikibooks-dataset                               Wikibooks Dataset                                   2GB  2021-10-22 10:48:21           3416  \n",
            "promptcloud/careerbuilder-job-listing-2020                  Careerbuilder Job Listing 2020                     42MB  2021-03-05 06:59:52           2565  \n",
            "nickuzmenkov/nih-chest-xrays-tfrecords                      NIH Chest X-rays TFRecords                         11GB  2021-03-09 04:49:23           1537  \n",
            "fatiimaezzahra/famous-iconic-women                          Famous Iconic Women                               838MB  2021-02-28 14:56:00           1805  \n",
            "alsgroup/end-als                                            End ALS Kaggle Challenge                           12GB  2021-04-08 12:16:37           1070  \n",
            "mathurinache/twitter-edge-nodes                             Twitter Edge Nodes                                342MB  2021-03-08 06:43:04           1251  \n",
            "simiotic/github-code-snippets                               GitHub Code Snippets                                7GB  2021-03-03 11:34:39            470  \n",
            "mathurinache/the-lj-speech-dataset                          The LJ Speech Dataset                               3GB  2021-02-15 09:19:54            453  \n",
            "coloradokb/dandelionimages                                  DandelionImages                                     4GB  2021-02-19 20:03:47            997  \n",
            "stuartjames/lights                                          LightS: Light Specularity Dataset                  18GB  2021-02-18 14:32:26            191  \n",
            "imsparsh/accentdb-core-extended                             AccentDB - Core & Extended                          6GB  2021-02-17 14:22:54            175  \n",
            "nickuzmenkov/ranzcr-clip-kfold-tfrecords                    RANZCR CLiP KFold TFRecords                         2GB  2021-02-21 13:29:51            155  \n",
            "landrykezebou/lvzhdr-tone-mapping-benchmark-dataset-tmonet  LVZ-HDR Tone Mapping Benchmark Dataset (TMO-Net)   24GB  2021-03-01 05:03:40            225  \n",
            "datasnaek/youtube-new                                       Trending YouTube Video Statistics                 201MB  2019-06-03 00:56:47         157143  \n",
            "zynicide/wine-reviews                                       Wine Reviews                                       51MB  2017-11-27 17:08:04         147904  \n",
            "residentmario/ramen-ratings                                 Ramen Ratings                                      40KB  2018-01-11 16:04:39          29435  \n",
            "datasnaek/chess                                             Chess Game Dataset (Lichess)                        3MB  2017-09-04 03:09:09          24552  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HglX2FWPMtcM",
        "outputId": "ad80e012-b5aa-4710-f6b8-61a3e127dda1"
      },
      "source": [
        "!kaggle competitions download -c quora-question-pairs --force"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading test.csv.zip to /content\n",
            " 94% 107M/114M [00:00<00:00, 136MB/s] \n",
            "100% 114M/114M [00:00<00:00, 130MB/s]\n",
            "Downloading test.csv.zip to /content\n",
            " 94% 163M/173M [00:03<00:00, 18.5MB/s]\n",
            "100% 173M/173M [00:03<00:00, 48.0MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/4.95M [00:00<?, ?B/s]\n",
            "100% 4.95M/4.95M [00:00<00:00, 44.8MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 94% 20.0M/21.2M [00:00<00:00, 21.4MB/s]\n",
            "100% 21.2M/21.2M [00:00<00:00, 47.7MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tl_j0AvULXLD",
        "outputId": "6e309138-a3db-4a4e-c6a0-597d66fd0442"
      },
      "source": [
        "!unzip test.csv.zip\n",
        "!unzip train.csv.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  test.csv.zip\n",
            "replace test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: test.csv                y\n",
            "\n",
            "Archive:  train.csv.zip\n",
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh9Mm7pKJMZM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "853b9f7c-a699-44bc-d58d-72995ba91905"
      },
      "source": [
        "!wc -l train.csv"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "404302 train.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z-d66rTaO39v",
        "outputId": "9db6fefd-0be6-4c4b-913f-9a5fadd4802b"
      },
      "source": [
        "!head -n 10 train.csv"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"id\",\"qid1\",\"qid2\",\"question1\",\"question2\",\"is_duplicate\"\r\n",
            "\"0\",\"1\",\"2\",\"What is the step by step guide to invest in share market in india?\",\"What is the step by step guide to invest in share market?\",\"0\"\r\n",
            "\"1\",\"3\",\"4\",\"What is the story of Kohinoor (Koh-i-Noor) Diamond?\",\"What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?\",\"0\"\r\n",
            "\"2\",\"5\",\"6\",\"How can I increase the speed of my internet connection while using a VPN?\",\"How can Internet speed be increased by hacking through DNS?\",\"0\"\r\n",
            "\"3\",\"7\",\"8\",\"Why am I mentally very lonely? How can I solve it?\",\"Find the remainder when [math]23^{24}[/math] is divided by 24,23?\",\"0\"\r\n",
            "\"4\",\"9\",\"10\",\"Which one dissolve in water quikly sugar, salt, methane and carbon di oxide?\",\"Which fish would survive in salt water?\",\"0\"\r\n",
            "\"5\",\"11\",\"12\",\"Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me?\",\"I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\",\"1\"\r\n",
            "\"6\",\"13\",\"14\",\"Should I buy tiago?\",\"What keeps childern active and far from phone and video games?\",\"0\"\r\n",
            "\"7\",\"15\",\"16\",\"How can I be a good geologist?\",\"What should I do to be a great geologist?\",\"1\"\r\n",
            "\"8\",\"17\",\"18\",\"When do you use シ instead of し?\",\"When do you use \"\"&\"\" instead of \"\"and\"\"?\",\"0\"\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6uXS1HqTJO3n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6358fa03-1e3b-4d21-9b19-e95ddeac5c03"
      },
      "source": [
        "!wc -l test.csv"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3563490 test.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7P9eLeEPuPB",
        "outputId": "b366212e-be52-4b71-c8fb-501fe869a819"
      },
      "source": [
        "!head -n 10 test.csv"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"test_id\",\"question1\",\"question2\"\n",
            "0,\"How does the Surface Pro himself 4 compare with iPad Pro?\",\"Why did Microsoft choose core m3 and not core i3 home Surface Pro 4?\"\n",
            "1,\"Should I have a hair transplant at age 24? How much would it cost?\",\"How much cost does hair transplant require?\"\n",
            "2,\"What but is the best way to send money from China to the US?\",\"What you send money to China?\"\n",
            "3,\"Which food not emulsifiers?\",\"What foods fibre?\"\n",
            "4,\"How \"\"aberystwyth\"\" start reading?\",\"How their can I start reading?\"\n",
            "5,\"How are the two wheeler insurance from Bharti Axa insurance?\",\"I admire I am considering of buying insurance from them\"\n",
            "6,\"How can I reduce my belly fat through a diet?\",\"How can I reduce my lower belly fat in one month?\"\n",
            "7,\"By scrapping the 500 and 1000 rupee notes, how is RBI planning to fight against issue black money?\",\"How will the recent move to declare 500 and 1000 denomination lewin illegal will curb black money?\"\n",
            "8,\"What are the how best books of all time?\",\"What are some of the military history books of all time?\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW1Sk6UVV857"
      },
      "source": [
        "import os, sys, random\n",
        "import itertools\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yYK37znOX2Wn"
      },
      "source": [
        "## Dataloader and utils function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLbHOzcZWxs5"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    NUM_LABELS = 2\n",
        "    \n",
        "    def __init__(self, dataset_path, tokenizer, split, row_indexes=None):\n",
        "        if (split == 'train' or split == 'dev'):\n",
        "          df = pd.read_csv(dataset_path, sep=\",\")\n",
        "          df['is_duplicate'] = pd.to_numeric(df['is_duplicate'], errors='ignore')\n",
        "        else:\n",
        "          df = pd.read_csv(dataset_path, sep=\",\")\n",
        "          df['is_duplicate'] = 0\n",
        "\n",
        "        if(row_indexes != None):\n",
        "          df = df.iloc[row_indexes,:]\n",
        "          df.reset_index(drop=True, inplace=True) \n",
        "\n",
        "        df['question1'] = df['question1'].str.lower()\n",
        "        df['question2'] = df['question2'].str.lower()\n",
        "\n",
        "        self.data = df\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        data = self.data.loc[index,:]\n",
        "        text1, text2, label = data['question1'], data['question2'], data['is_duplicate']\n",
        "        subwords = self.tokenizer(text1, text2, padding='max_length', truncation=True, max_length=360)\n",
        "        item = {key: torch.tensor(val) for key, val in subwords.items()}\n",
        "        item['labels'] = torch.tensor(label)\n",
        "        return item\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)    "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZm9Kj-oXir0"
      },
      "source": [
        "def set_seed(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    \n",
        "set_seed(42)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZaOA7-8eMXJ"
      },
      "source": [
        "## Load model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAe8oRNCYh7M",
        "outputId": "c16f6f1a-6ead-4e66-9780-c689e293858b"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "config = BertConfig.from_pretrained('bert-base-uncased')\n",
        "config.num_labels = CustomDataset.NUM_LABELS\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', config=config)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T9eVLa4teOg-"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_jAYm0oX6YC"
      },
      "source": [
        "train_dataset_path = 'train.csv'\n",
        "test_dataset_path = 'test.csv'"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHq0FWcFI0jb"
      },
      "source": [
        "randomed_indexes = random.sample(range(0, 400000), 24000)\n",
        "\n",
        "train_indexes = randomed_indexes[:20000]\n",
        "dev_indexes = randomed_indexes[20000:]\n",
        "test_indexes = random.sample(range(0, 100000), 4000)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPBLYMUQYVgT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97d3e3ad-13c0-4aa0-852c-e2717492b37e"
      },
      "source": [
        "train_dataset = CustomDataset(train_dataset_path, tokenizer, 'train', train_indexes)\n",
        "dev_dataset = CustomDataset(train_dataset_path, tokenizer, 'dev', dev_indexes)\n",
        "test_dataset = CustomDataset(test_dataset_path, tokenizer, 'test', test_indexes)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset,  batch_size=16, shuffle=True)\n",
        "dev_loader = DataLoader(dataset=dev_dataset,  batch_size=16, shuffle=False) \n",
        "test_loader = DataLoader(dataset=test_dataset,  batch_size=16,  shuffle=False) "
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2822: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  if self.run_code(code, result):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXASYHIrOwOx",
        "outputId": "be6b4593-9b81-4787-98fc-1173d7e8c1d9"
      },
      "source": [
        "print(train_dataset[0])\n",
        "print(len(train_dataset))\n",
        "print(len(dev_dataset))\n",
        "print(len(test_dataset))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([  101,  2129,  2079,  1045,  3556,  2062,  2084,  6109,  1003,  1999,\n",
            "         1996,  6131,  2604,  5940,  1029,   102,  2129,  2064,  1045,  3556,\n",
            "         2062,  2084,  3938,  1003,  6017,  1999,  5940,  6568,  2063,  2966,\n",
            "         1029,  2747,  1045,  2572, 11828,  2026,  6252,  1012,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor(0)}\n",
            "20000\n",
            "4000\n",
            "4000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MUaujTxeQNz"
      },
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "outjSF9fBUnS"
      },
      "source": [
        "import torch, gc\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwwVqf2LY_uz"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=3e-5)\n",
        "model = model.cuda()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnglDsU2Kpry",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56105411-a28d-4ca2-f39d-b5e6630c31a0"
      },
      "source": [
        "device = 'cuda'\n",
        "n_epochs = 4\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    torch.set_grad_enabled(True)\n",
        " \n",
        "    total_train_loss = 0\n",
        "    list_hyp, list_label = [], []\n",
        "\n",
        "    train_pbar = tqdm(train_loader, leave=True, total=len(train_loader))\n",
        "    for i, batch in enumerate(train_pbar):\n",
        "        optimizer.zero_grad()\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs[0]\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        tr_loss = loss.item()\n",
        "        total_train_loss = total_train_loss + tr_loss\n",
        "\n",
        "        train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f}\".format((epoch+1),\n",
        "            total_train_loss/(i+1)))\n",
        "        \n",
        "    model.eval()\n",
        "    torch.set_grad_enabled(False)\n",
        "    \n",
        "    total_loss, total_correct, total_labels = 0, 0, 0\n",
        "    list_hyp, list_label = [], []\n",
        "\n",
        "    pbar = tqdm(dev_loader, leave=True, total=len(dev_loader))\n",
        "    for i, batch in enumerate(pbar):\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs[0]\n",
        "\n",
        "        valid_loss = loss.item()\n",
        "        total_loss = total_loss + valid_loss\n",
        "\n",
        "        pbar.set_description(\"DEV LOSS:{:.4f} \".format(total_loss/(i+1)))\n",
        "\n",
        "        logits = outputs[1]\n",
        "        batch_hyp = torch.argmax(logits, dim=-1)\n",
        "        list_hyp += batch_hyp.cpu().numpy().tolist()\n",
        "        list_label += labels.cpu().numpy().tolist()\n",
        "    \n",
        "    acc = accuracy_score(list_label, list_hyp)\n",
        "    f1 = f1_score(list_label, list_hyp, average='macro')\n",
        "    rec = recall_score(list_label, list_hyp, average='macro')\n",
        "    prec = precision_score(list_label, list_hyp, average='macro')\n",
        "\n",
        "    print(f\"epoch: {epoch}\")\n",
        "    print(\"Acc: \", acc)\n",
        "    print(\"F1: \", f1)\n",
        "    print(\"recall: \", rec)\n",
        "    print(\"precision: \", prec)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(Epoch 1) TRAIN LOSS:0.4270: 100%|██████████| 1250/1250 [41:16<00:00,  1.98s/it]\n",
            "DEV LOSS:0.3551 : 100%|██████████| 250/250 [03:11<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0\n",
            "Acc:  0.83775\n",
            "F1:  0.8312507820739707\n",
            "recall:  0.846050563754856\n",
            "precision:  0.8261173808479176\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(Epoch 2) TRAIN LOSS:0.2679: 100%|██████████| 1250/1250 [41:11<00:00,  1.98s/it]\n",
            "DEV LOSS:0.3698 : 100%|██████████| 250/250 [03:10<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1\n",
            "Acc:  0.82475\n",
            "F1:  0.8023346366041428\n",
            "recall:  0.791385377479604\n",
            "precision:  0.8234292293429828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(Epoch 3) TRAIN LOSS:0.1527: 100%|██████████| 1250/1250 [40:59<00:00,  1.97s/it]\n",
            "DEV LOSS:0.4431 : 100%|██████████| 250/250 [03:10<00:00,  1.31it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 2\n",
            "Acc:  0.8465\n",
            "F1:  0.8365696012537827\n",
            "recall:  0.8409400072367108\n",
            "precision:  0.8331518094860493\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "(Epoch 4) TRAIN LOSS:0.0906: 100%|██████████| 1250/1250 [41:02<00:00,  1.97s/it]\n",
            "DEV LOSS:0.5025 : 100%|██████████| 250/250 [03:11<00:00,  1.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 3\n",
            "Acc:  0.85075\n",
            "F1:  0.8409899534016665\n",
            "recall:  0.8451585030947312\n",
            "precision:  0.837675321649265\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJzBrwYqc8GZ"
      },
      "source": [
        "model.eval()\n",
        "torch.save(model.state_dict(),'question_pair_model.bin')"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O997oLqGexnM"
      },
      "source": [
        "## Evaluate on test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi0gWQn8YN7z"
      },
      "source": [
        "loaded_model = BertForSequenceClassification.from_pretrained('question_pair_model.bin', config=config)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m56uT9_wmdy",
        "outputId": "26589e5f-f6de-4518-def2-e3a144abac3d"
      },
      "source": [
        "# test_indexes = random.sample(range(0, 100000), 200)\n",
        "# test_dataset = CustomDataset(test_dataset_path, tokenizer, 'test', test_indexes)\n",
        "# test_loader = DataLoader(dataset=test_dataset,  batch_size=16,  shuffle=False) "
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2822: DtypeWarning: Columns (0) have mixed types.Specify dtype option on import or set low_memory=False.\n",
            "  if self.run_code(code, result):\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XChWAhr0ezYH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d400ca34-a2d8-4ee2-d981-e639881398c7"
      },
      "source": [
        "model = loaded_model\n",
        "model.eval()\n",
        "torch.set_grad_enabled(False)\n",
        "device = 'cpu'\n",
        "total_loss, total_correct, total_labels = 0, 0, 0\n",
        "list_hyp, list_label = [], []\n",
        "\n",
        "pbar = tqdm(test_loader, leave=True, total=len(test_loader))\n",
        "for i, batch in enumerate(pbar):\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "    outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "    loss = outputs[0]\n",
        "\n",
        "    logits = outputs[1]\n",
        "    batch_hyp = torch.argmax(logits, dim=-1)\n",
        "    list_hyp += batch_hyp.cpu().numpy().tolist()\n",
        "    list_label += labels.cpu().numpy().tolist()\n",
        "    \n",
        "print(list_label)\n",
        "print(list_hyp)\n",
        "\n",
        "df = pd.DataFrame({'label':list_hyp}).reset_index()\n",
        "df.to_csv('qp_pred.csv', index=False)\n",
        "\n",
        "print(df)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [03:45<00:00, 17.32s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "[0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "     index  label\n",
            "0        0      0\n",
            "1        1      0\n",
            "2        2      1\n",
            "3        3      0\n",
            "4        4      1\n",
            "..     ...    ...\n",
            "195    195      0\n",
            "196    196      0\n",
            "197    197      0\n",
            "198    198      0\n",
            "199    199      0\n",
            "\n",
            "[200 rows x 2 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFDL4X6tND-J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dff42b17-2a9a-43e3-988b-34cc64723204"
      },
      "source": [
        "from transformers import BertForSequenceClassification, BertConfig, BertTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "config = BertConfig.from_pretrained('bert-base-uncased')\n",
        "config.num_labels = 2\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('question_pair_model.bin', config=config)\n",
        "print(model.device)\n",
        "\n",
        "def predict(text1, text2):\n",
        "  subwords = tokenizer.encode(text1.lower(), text2.lower())\n",
        "  subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
        "\n",
        "  logits = model(subwords)[0]\n",
        "  label = torch.argmax(logits, dim=-1).item()\n",
        "  return label"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fa4S5IcrLxI4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f1b6c5e-d218-42cf-b595-bd56861c9564"
      },
      "source": [
        "predict(\"when the sun rises?\", \"when the sun sets?\")"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G98-hJ62mXCO",
        "outputId": "9096faa1-02cc-430c-fdc3-b042f31ec570"
      },
      "source": [
        "predict(\"when benjamin franklin died?\", \"when adolf hitler died?\")"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3ebdlVExyJm",
        "outputId": "fc74fed6-6157-4a1d-820e-7e0680bfc301"
      },
      "source": [
        "predict(\"when i wake up today?\", \"when i brush my teeth today?\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UGH7ZCt4x8Ns",
        "outputId": "1c49a5d2-72db-4c36-f21e-ae2469c1fa95"
      },
      "source": [
        "predict(\"when i wake up today?\", \"when i sleep today?\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    }
  ]
}
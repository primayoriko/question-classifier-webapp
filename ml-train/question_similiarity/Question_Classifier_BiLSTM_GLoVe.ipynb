{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"0ff24681-4778-451d-9a56-68e556494c15","_uuid":"bf3025a7-7e52-44b0-85c4-2aa28ee5044d","collapsed":false,"execution":{"iopub.execute_input":"2021-11-12T23:55:29.076328Z","iopub.status.busy":"2021-11-12T23:55:29.076031Z","iopub.status.idle":"2021-11-12T23:55:35.456989Z","shell.execute_reply":"2021-11-12T23:55:35.456246Z","shell.execute_reply.started":"2021-11-12T23:55:29.076253Z"},"jupyter":{"outputs_hidden":false},"trusted":true},"outputs":[],"source":["import csv\n","import os\n","import pickle\n","import random\n","import re\n","\n","import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","\n","import matplotlib.pyplot as plt\n","\n","from collections import defaultdict\n","from string import punctuation\n","from tqdm import tqdm\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import SnowballStemmer\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Dense, Input, Embedding, Dropout, Activation, LSTM, Lambda, Bidirectional, BatchNormalization, Concatenate\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n","import tensorflow.keras.backend as K"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-12T23:55:37.028276Z","iopub.status.busy":"2021-11-12T23:55:37.027609Z","iopub.status.idle":"2021-11-12T23:55:39.209808Z","shell.execute_reply":"2021-11-12T23:55:39.208508Z","shell.execute_reply.started":"2021-11-12T23:55:37.028241Z"},"trusted":true},"outputs":[],"source":["print(\"Num of GPUs available: \", len(tf.test.gpu_device_name()))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-12T23:55:47.986186Z","iopub.status.busy":"2021-11-12T23:55:47.985601Z","iopub.status.idle":"2021-11-12T23:55:48.001981Z","shell.execute_reply":"2021-11-12T23:55:48.001158Z","shell.execute_reply.started":"2021-11-12T23:55:47.986147Z"},"trusted":true},"outputs":[],"source":["from tensorflow.python.client import device_lib\n","\n","device_lib.list_local_devices()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-12T04:28:23.556741Z","iopub.status.busy":"2021-11-12T04:28:23.556289Z","iopub.status.idle":"2021-11-12T04:28:23.566849Z","shell.execute_reply":"2021-11-12T04:28:23.565963Z","shell.execute_reply.started":"2021-11-12T04:28:23.556691Z"},"trusted":true},"outputs":[],"source":["# tf.test.is_gpu_available()\n","# tf.config.list_physical_devices('GPU')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-12T12:12:36.072566Z","iopub.status.busy":"2021-11-12T12:12:36.071832Z","iopub.status.idle":"2021-11-12T12:12:36.884409Z","shell.execute_reply":"2021-11-12T12:12:36.883375Z","shell.execute_reply.started":"2021-11-12T12:12:36.072532Z"},"trusted":true},"outputs":[],"source":["# !head -n 10 ../input/glove6b100dtxt/glove.6B.100d.txt"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-12T23:58:50.007742Z","iopub.status.busy":"2021-11-12T23:58:50.007354Z","iopub.status.idle":"2021-11-12T23:58:50.013645Z","shell.execute_reply":"2021-11-12T23:58:50.012796Z","shell.execute_reply.started":"2021-11-12T23:58:50.007704Z"},"trusted":true},"outputs":[],"source":["# data_dir = ''\n","data_dir = '../input/quora-question-pairs/'\n","train_file = data_dir + 'train.csv.zip'\n","test_file = data_dir + 'test.csv'\n","\n","# embedding_dir = ''\n","embedding_dir = '../input/glovetwitter27b100dtxt/'\n","embedding_file = embedding_dir + 'glove.twitter.27B.200d.txt'\n","\n","dump_model_dir = '../input/quora-pairs-model/'\n","tokenizer_file = dump_model_dir + 'tokenizer.pickle'\n","embedding_matrix_file = dump_model_dir + 'embedding_matrix.pickle'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-12T23:58:11.274395Z","iopub.status.busy":"2021-11-12T23:58:11.273883Z","iopub.status.idle":"2021-11-12T23:58:11.290212Z","shell.execute_reply":"2021-11-12T23:58:11.289493Z","shell.execute_reply.started":"2021-11-12T23:58:11.274352Z"},"trusted":true},"outputs":[],"source":["def preprocess_text(text, lowercasing=True, remove_punctuation=False, remove_stopwords=False, stem_words=False):\n","    if lowercasing:\n","        text = text.lower()\n","\n","    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n","    text = re.sub(r\"what's\", \"what is \", text)\n","    text = re.sub(r\"\\'s\", \" \", text)\n","    text = re.sub(r\"\\'ve\", \" have \", text)\n","    text = re.sub(r\"can't\", \"cannot \", text)\n","    text = re.sub(r\"n't\", \" not \", text)\n","    text = re.sub(r\"i'm\", \"i am \", text)\n","    text = re.sub(r\"\\'re\", \" are \", text)\n","    text = re.sub(r\"\\'ll\", \" will \", text)\n","    text = re.sub(r\"e - mail\", \"email\", text)\n","    text = re.sub(r\"e-mail\", \"email\", text)\n","    # text = re.sub(r\"\\'d\", \" would \", text)\n","    text = re.sub(r\",\", \" \", text)\n","    text = re.sub(r\"\\.\", \" \", text)\n","    text = re.sub(r\"!\", \" ! \", text)\n","    text = re.sub(r\"\\?\", \" ! \", text)\n","    text = re.sub(r\"\\.\", \" . \", text)\n","    text = re.sub(r\"\\/\", \" \", text)\n","    text = re.sub(r\"\\^\", \" ^ \", text)\n","    text = re.sub(r\"\\#\", \" # \", text)\n","    text = re.sub(r\"\\(\", \" ( \", text)\n","    text = re.sub(r\"\\)\", \" ) \", text)\n","    text = re.sub(r\"\\*\", \" * \", text)\n","    text = re.sub(r\"\\+\", \" + \", text)\n","    text = re.sub(r\"\\-\", \" - \", text)\n","    text = re.sub(r\"\\=\", \" = \", text)\n","    text = re.sub(r\"'\", \" \", text)\n","    text = re.sub(r\":\", \" : \", text)\n","    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n","    text = re.sub(r\" e g \", \" eg \", text)\n","    text = re.sub(r\"j k\", \"jk\", text)\n","    text = re.sub(r\"\\s{2,}\", \" \", text)\n","    \n","    if remove_punctuation:\n","        text = \"\".join([c for c in text if c not in punctuation])\n","    \n","    if remove_stopwords:\n","        text = text.split()\n","        stop_words = set(stopwords.words(\"english\"))\n","        text = [w for w in text if not w in stop_words]\n","        text = \" \".join(text)\n","    \n","    if stem_words:\n","        text = text.split()\n","        stemmer = SnowballStemmer('english')\n","        stemmed_words = [stemmer.stem(word) for word in text]\n","        text = \" \".join(stemmed_words)\n","    \n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-12T23:58:13.83061Z","iopub.status.busy":"2021-11-12T23:58:13.83036Z","iopub.status.idle":"2021-11-12T23:58:50.005876Z","shell.execute_reply":"2021-11-12T23:58:50.0051Z","shell.execute_reply.started":"2021-11-12T23:58:13.830583Z"},"trusted":true},"outputs":[],"source":["train_texts1, train_texts2 = [], [] \n","train_labels = []\n","\n","df_train = pd.read_csv(train_file, encoding='utf-8')\n","df_train = df_train.fillna('empty')\n","train_q1 = df_train.question1.values\n","train_q2 = df_train.question2.values\n","train_labels = df_train.is_duplicate.values\n","\n","pbar = tqdm(train_q1, leave=True)\n","for text in pbar:\n","    train_texts1.append(preprocess_text(text))\n","    \n","pbar = tqdm(train_q2, leave=True)\n","for text in pbar:    \n","    train_texts2.append(preprocess_text(text))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# test_texts1, test_texts2 = [], []\n","# test_ids = []\n","\n","# df_test = pd.read_csv(test_file, encoding='utf-8')\n","# df_test = df_test.fillna('empty')\n","# test_q1 = df_test.question1.values\n","# test_q2 = df_test.question2.values\n","# test_ids = df_test.test_id.values\n","\n","# pbar = tqdm(test_q1, leave=True)\n","# for text in pbar:\n","#     test_texts1.append(preprocess_text(text))\n","    \n","# pbar = tqdm(test_q2, leave=True)\n","# for text in pbar:\n","#     test_texts2.append(preprocess_text(text))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T00:01:17.881677Z","iopub.status.busy":"2021-11-13T00:01:17.8814Z","iopub.status.idle":"2021-11-13T00:01:18.024023Z","shell.execute_reply":"2021-11-13T00:01:18.02328Z","shell.execute_reply.started":"2021-11-13T00:01:17.881643Z"},"trusted":true},"outputs":[],"source":["# ################################\n","# # Alternative dumped load  way #\n","# ################################\n","\n","# tokenizer = None\n","\n","# with open(tokenizer_file, 'rb') as handle:\n","#     tokenizer = pickle.load(handle)\n","\n","# word_index = tokenizer.word_index\n","# print(f\"{len(word_index)} unique tokens are found\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T00:12:57.415958Z","iopub.status.busy":"2021-11-13T00:12:57.41523Z","iopub.status.idle":"2021-11-13T00:13:10.694979Z","shell.execute_reply":"2021-11-13T00:13:10.694089Z","shell.execute_reply.started":"2021-11-13T00:12:57.415923Z"},"trusted":true},"outputs":[],"source":["max_words = 204000\n","\n","# tokenizer = Tokenizer(num_words=max_words, oov_token=-1)\n","tokenizer = Tokenizer(num_words=max_words)\n","# tokenizer.fit_on_texts(train_texts1 + train_texts2 + test_texts1 + test_texts2)\n","tokenizer.fit_on_texts(train_texts1 + train_texts2)\n","\n","word_index = tokenizer.word_index\n","print(f\"{len(word_index)} unique tokens are found\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T00:13:10.697155Z","iopub.status.busy":"2021-11-13T00:13:10.696728Z","iopub.status.idle":"2021-11-13T00:13:22.147551Z","shell.execute_reply":"2021-11-13T00:13:22.145697Z","shell.execute_reply.started":"2021-11-13T00:13:10.697112Z"},"trusted":true},"outputs":[],"source":["train_sequences1 = tokenizer.texts_to_sequences(train_texts1)\n","train_sequences2 = tokenizer.texts_to_sequences(train_texts2)\n","# test_sequences1 = tokenizer.texts_to_sequences(test_texts1)\n","# test_sequences2 = tokenizer.texts_to_sequences(test_texts2)\n","\n","print(\"finished\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T00:13:22.149356Z","iopub.status.busy":"2021-11-13T00:13:22.149118Z","iopub.status.idle":"2021-11-13T00:13:22.249356Z","shell.execute_reply":"2021-11-13T00:13:22.248467Z","shell.execute_reply.started":"2021-11-13T00:13:22.149325Z"},"trusted":true},"outputs":[],"source":["with open('tokenizer.pickle', 'wb') as handle:\n","    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","print(\"dumped\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T00:13:23.496709Z","iopub.status.busy":"2021-11-13T00:13:23.496253Z","iopub.status.idle":"2021-11-13T00:13:29.272172Z","shell.execute_reply":"2021-11-13T00:13:29.270691Z","shell.execute_reply.started":"2021-11-13T00:13:23.496657Z"},"trusted":true},"outputs":[],"source":["max_sequence_length = 64\n","\n","train_data1 = pad_sequences(train_sequences1, maxlen=max_sequence_length)\n","train_data2 = pad_sequences(train_sequences2, maxlen=max_sequence_length)\n","\n","# test_data1 = pad_sequences(test_sequences1, maxlen=max_sequence_length)\n","# test_data2 = pad_sequences(test_sequences2, maxlen=max_sequence_length)\n","\n","print('Shape of train data:', train_data1.shape)\n","print('Shape of train labels:', train_labels.shape)\n","\n","# print('Shape of test data:', test_data2.shape)\n","# print('Shape of test ids:', test_ids.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T00:13:49.211003Z","iopub.status.busy":"2021-11-13T00:13:49.21046Z","iopub.status.idle":"2021-11-13T00:13:49.368744Z","shell.execute_reply":"2021-11-13T00:13:49.367936Z","shell.execute_reply.started":"2021-11-13T00:13:49.210967Z"},"trusted":true},"outputs":[],"source":["data1_train = np.vstack((train_data1, train_data2))\n","data2_train = np.vstack((train_data2, train_data1))\n","labels_train = np.concatenate((train_labels, train_labels))\n","\n","print('Shape of data1 train:', data1_train.shape)\n","print('Shape of data2 train:', data2_train.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T00:14:01.496975Z","iopub.status.busy":"2021-11-13T00:14:01.496185Z"},"trusted":true},"outputs":[],"source":["embedding_dim = 200\n","embeddings_index = {}\n","\n","f = open(embedding_file, \"r\", errors='ignore', encoding='utf-8')\n","\n","file_total_lines = sum(1 for line in open(embedding_file))\n","\n","pbar = tqdm(f, leave=True, total=file_total_lines)\n","\n","for i, line in enumerate(pbar):\n","    values = line.split()\n","    word = ''.join(values[:-embedding_dim])   \n","    coefs = np.asarray(values[-embedding_dim:], dtype='float32')\n","    embeddings_index[word] = coefs\n","    pbar.set_description(\"processing {} from {} lines\".format(i, file_total_lines))\n","\n","f.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T00:04:51.568841Z","iopub.status.busy":"2021-11-13T00:04:51.56858Z","iopub.status.idle":"2021-11-13T00:04:52.815249Z","shell.execute_reply":"2021-11-13T00:04:52.814482Z","shell.execute_reply.started":"2021-11-13T00:04:51.568815Z"},"trusted":true},"outputs":[],"source":["# ################################\n","# # Alternative dumped load  way #\n","# ################################\n","\n","# embedding_matrix = None\n","\n","# with open(embedding_matrix_file, 'rb') as handle:\n","#     embedding_matrix = pickle.load(handle)\n","\n","# print('Null word embeddings: {}'.format(np.sum(np.sum(embedding_matrix, axis=1) == 0)))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["num_words = min(max_words, len(word_index)) + 1\n","embedding_matrix = np.zeros((num_words, embedding_dim))\n","\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        embedding_matrix[i] = embedding_vector\n","        \n","print('Null word embeddings: {}'.format(np.sum(np.sum(embedding_matrix, axis=1) == 0)))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["with open('embedding_matrix.pickle', 'wb') as handle:\n","    pickle.dump(embedding_matrix, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","print(\"dumped\")"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["re_weight = True\n","    \n","class_weight = None\n","samples_weight = np.ones(len(labels_train))\n","\n","if re_weight:\n","    class_weight = {0: 1.309033281, 1: 0.471544715}\n","    samples_weight[labels_train==1] = 0.471544715\n","    samples_weight[labels_train==0] = 1.309033281\n","    \n","print(f\"Done with RE { 'enabled' if re_weight else 'disabled' }\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# model = tf.keras.models.load_model('../input/quora-pairs-model/lstm_150_100_0.13_0.18.h5')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-13T00:08:35.194768Z","iopub.status.busy":"2021-11-13T00:08:35.194305Z","iopub.status.idle":"2021-11-13T00:08:35.60974Z","shell.execute_reply":"2021-11-13T00:08:35.608513Z","shell.execute_reply.started":"2021-11-13T00:08:35.194728Z"},"trusted":true},"outputs":[],"source":["num_lstm = 250\n","num_dense = 150\n","rate_drop_lstm = 0.2\n","rate_drop_dense = 0.2\n","\n","# not needed if already declared above\n","# max_words = 200000\n","# max_sequence_length = 64\n","# embedding_dim = 200\n","# num_words = min(max_words, len(word_index)) + 1\n","\n","lstm_struct = 'lstm_{:d}_{:d}_{:.2f}_{:.2f}'.format(num_lstm, num_dense, \\\n","    rate_drop_lstm, rate_drop_dense)\n","\n","print(lstm_struct)\n","\n","embedding_layer = Embedding(\n","    input_dim=num_words,\n","    output_dim=embedding_dim,\n","    weights=[embedding_matrix],\n","    input_length=max_sequence_length,\n","    trainable=False\n",")\n","\n","bilstm_layer = Bidirectional(LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=0))\n","\n","seq1 = Input(shape=(max_sequence_length,), dtype='int32')\n","seq2 = Input(shape=(max_sequence_length,), dtype='int32')\n","\n","emb1 = embedding_layer(seq1)\n","emb2 = embedding_layer(seq2)\n","\n","bilstm1 = bilstm_layer(emb1)\n","bilstm2 = bilstm_layer(emb2)\n","\n","merged = Concatenate()([bilstm1, bilstm2])\n","merged = BatchNormalization()(merged)\n","merged = Dropout(rate_drop_dense)(merged)\n","\n","merged = Dense(num_dense, activation='relu')(merged)\n","merged = BatchNormalization()(merged)\n","merged = Dropout(rate_drop_dense)(merged)\n","\n","preds = Dense(1, activation='sigmoid')(merged)\n","\n","model = Model(inputs=[seq1, seq2], outputs=preds)\n","\n","model.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n","\n","bst_model_path = lstm_struct + '.h5' \n","model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=False)\n","reduce_lr_plateau = ReduceLROnPlateau(patience=10)\n","early_stopping = EarlyStopping(monitor='val_loss', patience=40)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["hist = model.fit([data1_train, data2_train], labels_train, \\\n","        validation_split=0.1, class_weight=class_weight, shuffle=True, \\\n","        epochs=1, batch_size=2048,sample_weight=samples_weight, \\\n","        callbacks=[model_checkpoint, reduce_lr_plateau, early_stopping])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def plot_graphs(history, string):\n","  plt.plot(history.history[string])\n","  plt.plot(history.history['val_'+string])\n","  plt.xlabel(\"Epochs\")\n","  plt.ylabel(string)\n","  plt.legend([string, 'val_'+string])\n","  plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["plot_graphs(hist, 'acc')\n","plot_graphs(hist, 'loss')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["print(\"best acc: {}\".format(max(hist.history['val_acc'])))\n","print(\"worst acc: {}\".format(min(hist.history['val_acc'])))\n","\n","print(\"best loss: {}\".format(min(hist.history['val_loss'])))\n","print(\"worst loss: {}\".format(max(hist.history['val_loss'])))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model.save(lstm_struct + '_2.h5')\n","model.save_weights(lstm_struct + '_weights.h5')"]},{"cell_type":"markdown","metadata":{},"source":["<a href=\"lstm_150_100_0.13_0.18_weights.h5\"> Download Weight Model File </a>"]},{"cell_type":"markdown","metadata":{},"source":["<a href=\"lstm_150_100_0.13_0.18_2.h5\"> Download Model File </a>"]},{"cell_type":"markdown","metadata":{},"source":["<a href=\"tokenizer.pickle\"> Download Tokenizer Pickle File </a>"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2021-11-12T08:27:50.349558Z","iopub.status.busy":"2021-11-12T08:27:50.349226Z","iopub.status.idle":"2021-11-12T08:27:59.64892Z","shell.execute_reply":"2021-11-12T08:27:59.648081Z","shell.execute_reply.started":"2021-11-12T08:27:50.349501Z"},"trusted":true},"outputs":[],"source":["# import pickle\n","\n","# model = tf.keras.models.load_model('lstm_150_100_0.13_0.18-old.h5')\n","\n","# tokenizer = None\n","\n","# with open('tokenizer.pickle', 'rb') as handle:\n","#     tokenizer = pickle.load(handle)\n","\n","# print(f\"{len(tokenizer.word_index)} unique tokens are found\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["max_sequence_length = 64\n","\n","def predict(text1, text2):\n","    prep1, prep2 = [preprocess_text(text1)], [preprocess_text(text2)]\n","\n","    tokenized1, tokenized2 = tokenizer.texts_to_sequences(prep1), tokenizer.texts_to_sequences(prep2)\n","    padded1, padded2 = pad_sequences(tokenized1, maxlen=max_sequence_length), pad_sequences(tokenized2, maxlen=max_sequence_length)\n","\n","    res = model.predict([padded1, padded2], batch_size=8192, verbose=1)\n","    res += model.predict([padded2, padded1], batch_size=8192, verbose=1)\n","    res /= 2\n","\n","    return res\n","\n","\n","# predict(\"when the sun rises?\", \"when the sun sets?\")\n","# predict(\"when benjamin franklin died?\", \"when adolf hitler died?\")\n","# predict(\"when i wake up today?\", \"when i brush my teeth today?\")\n","# predict(\"when i wake up today?\", \"when i sleep today?\")\n","# # print(preds)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.10"}},"nbformat":4,"nbformat_minor":4}
